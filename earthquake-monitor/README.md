# ğŸŒ Earthquake Monitor API

A comprehensive real-time earthquake monitoring system built with Clean Architecture principles, featuring USGS data ingestion, secure API access, and WebSocket real-time updates.

[![Tests](https://github.com/L3pereira/ndgms/workflows/Tests/badge.svg)](https://github.com/L3pereira/ndgms/actions)
[![Coverage](https://codecov.io/gh/L3pereira/ndgms/branch/main/graph/badge.svg)](https://codecov.io/gh/L3pereira/ndgms)
[![Python](https://img.shields.io/badge/python-3.11%2B-blue)](https://www.python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.116.2-009688.svg)](https://fastapi.tiangolo.com)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)

## ğŸš€ Features

### **Core Capabilities**
- **ğŸŒ Real-time Data Ingestion** - Automated USGS earthquake data collection
- **ğŸ”’ Secure RESTful API** - OAuth2 JWT authentication with role-based access
- **ğŸ“¡ WebSocket Support** - Live earthquake notifications and updates
- **ğŸ” Advanced Filtering** - Search by magnitude, location, time, and source with PostGIS spatial queries
- **ğŸ“„ Pagination** - Efficient handling of large datasets
- **âš¡ High Performance** - Async/await throughout with connection pooling

### **Architecture Highlights**
- **ğŸ—ï¸ Clean Architecture** - Domain-driven design with proper separation of concerns
- **ğŸ§ª Comprehensive Testing** - 95%+ test coverage with unit and integration tests
- **ğŸ³ Containerized** - Docker support with multi-stage builds
- **ğŸ“Š Monitoring Ready** - Structured logging and health checks
- **ğŸ”§ Developer Experience** - Pre-commit hooks, linting, and type checking
- **ğŸŒ Geospatial Database** - PostGIS integration for accurate spatial queries and indexing

## ğŸ“‹ Table of Contents

- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [API Documentation](#-api-documentation)
- [Architecture](#-architecture)
- [Configuration](#-configuration)
- [Development](#-development)
- [Testing](#-testing)
- [Deployment](#-deployment)
- [Contributing](#-contributing)

## âš¡ Quick Start

### 1. **Clone and Setup**
```bash
git clone https://github.com/L3pereira/ndgms.git
cd ndgms/earthquake-monitor

# Setup virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2. **Database Setup**
```bash
# Start PostgreSQL (using Docker)
docker-compose up -d postgres

# Run migrations
alembic upgrade head
```

### 3. **Start the API**
```bash
# Development server
uvicorn src.presentation.main:app --reload --host 0.0.0.0 --port 8000

# Or using Docker
docker-compose up --build
```

### 4. **Access the API**
- **API Documentation**: http://localhost:8000/docs
- **Alternative Docs**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/health

## ğŸ› ï¸ Installation

### **System Requirements**
- Python 3.12+
- PostgreSQL 15+ with PostGIS 3.3+ extension
- Docker & Docker Compose (optional)

### **Local Development Setup**

1. **Environment Configuration**
```bash
# Copy environment template
cp .env.example .env

# Edit configuration
vim .env
```

2. **Install Dependencies**
```bash
# Using Poetry (recommended)
poetry install

# Or using pip
pip install -r requirements.txt
```

3. **Database Migration**
```bash
# Run database migrations
alembic upgrade head

# Create test data (optional)
python scripts/ingestion/usgs_ingestion.py
```

### **Docker Setup**

```bash
# Development environment
docker-compose -f docker/docker-compose.dev.yml up --build

# Production environment
docker-compose -f docker/docker-compose.yml up --build
```

## ğŸ“š API Documentation

### **Interactive Documentation**
- **Swagger UI**: [http://localhost:8000/docs](http://localhost:8000/docs)
- **ReDoc**: [http://localhost:8000/redoc](http://localhost:8000/redoc)
- **OpenAPI Schema**: [http://localhost:8000/openapi.json](http://localhost:8000/openapi.json)

### **Authentication Flow**

1. **Register a new user**:
```bash
curl -X POST "http://localhost:8000/api/v1/auth/register" \
  -H "Content-Type: application/json" \
  -d '{
    "email": "user@example.com",
    "username": "testuser",
    "password": "securepassword123",
    "full_name": "Test User"
  }'
```

2. **Login to get access token**:
```bash
curl -X POST "http://localhost:8000/api/v1/auth/login" \
  -H "Content-Type: application/json" \
  -d '{
    "email": "user@example.com",
    "password": "securepassword123"
  }'
```

3. **Use token for API requests**:
```bash
curl -X GET "http://localhost:8000/api/v1/earthquakes" \
  -H "Authorization: Bearer YOUR_ACCESS_TOKEN"
```

### **Complete API Reference**

#### **ğŸ” Authentication Endpoints** (`/api/v1/auth`)
| Method | Endpoint | Description | Auth Required |
|--------|----------|-------------|---------------|
| `POST` | `/register` | Register new user account | âŒ |
| `POST` | `/login` | User authentication (returns JWT tokens) | âŒ |
| `POST` | `/refresh` | Refresh access token using refresh token | âŒ |
| `GET` | `/me` | Get current authenticated user info | âœ… |
| `POST` | `/logout` | Logout current user | âœ… |
| `GET` | `/verify` | Verify if current token is valid | âœ… |

#### **ğŸŒ Earthquake Data Endpoints** (`/api/v1/earthquakes`)
| Method | Endpoint | Description | Auth Required |
|--------|----------|-------------|---------------|
| `GET` | `/` | List earthquakes with advanced filtering & pagination | âœ… |
| `GET` | `/{earthquake_id}` | Get detailed earthquake information | âœ… |
| `POST` | `/` | Create earthquake record manually | âœ… |

#### **ğŸ“¥ Data Ingestion Endpoints** (`/api/v1/ingestion`)
| Method | Endpoint | Description | Auth Required |
|--------|----------|-------------|---------------|
| `POST` | `/trigger` | Manually trigger USGS data ingestion | âœ… |
| `GET` | `/sources` | Get available data sources (USGS feeds info) | âœ… |
| `GET` | `/status` | Get current ingestion system status | âœ… |

#### **ğŸ“¡ Real-time Communication**
| Method | Endpoint | Description | Auth Required |
|--------|----------|-------------|---------------|
| `WS` | `/api/v1/ws` | WebSocket endpoint for real-time updates | âœ… |

#### **ğŸ¥ System Health**
| Method | Endpoint | Description | Auth Required |
|--------|----------|-------------|---------------|
| `GET` | `/health` | System health check | âŒ |

### **Filtering Examples**

```bash
# Get earthquakes by magnitude range
GET /api/v1/earthquakes?min_magnitude=5.0&max_magnitude=7.0

# Filter by time range
GET /api/v1/earthquakes?start_time=2024-01-01T00:00:00Z&end_time=2024-01-31T23:59:59Z

# Geographic filtering using PostGIS (accurate distance within 100km)
GET /api/v1/earthquakes?latitude=34.0522&longitude=-118.2437&radius_km=100

# Pagination
GET /api/v1/earthquakes?limit=50&offset=100

# Combined filters
GET /api/v1/earthquakes?min_magnitude=6.0&source=USGS&limit=10
```

## ğŸ—ï¸ Architecture

### **System Architecture**

```mermaid
graph TB
    %% External Systems
    USGS["ğŸŒ USGS API<br/>GeoJSON Feeds & FDSNWS"]
    Client["ğŸ’» Frontend Client<br/>Web/Mobile App"]

    %% Presentation Layer
    subgraph "ğŸŒ Presentation Layer"
        API["ğŸ”Œ FastAPI<br/>REST Endpoints"]
        WS["ğŸ“¡ WebSocket<br/>Real-time Updates"]
        Auth["ğŸ” OAuth2/JWT<br/>Authentication"]
    end

    %% Application Layer
    subgraph "ğŸ”„ Application Layer"
        UC["âš™ï¸ Use Cases<br/>Business Logic"]
        Events["ğŸ“¨ Event System<br/>Publisher/Handlers"]
        DTO["ğŸ“¦ DTOs<br/>Data Transfer"]
    end

    %% Domain Layer
    subgraph "ğŸ¯ Domain Layer"
        Entities["ğŸ¢ Entities<br/>Earthquake, Location, Magnitude"]
        DomainEvents["ğŸ“¢ Domain Events<br/>EarthquakeDetected, HighMagnitudeAlert"]
        Repository["ğŸ“‹ Repository Interfaces<br/>Abstract Data Access"]
    end

    %% Infrastructure Layer
    subgraph "ğŸ”Œ Infrastructure Layer"
        PostgreSQL["ğŸ—„ï¸ PostgreSQL + PostGIS<br/>Spatial Database"]
        USGSService["ğŸŒ USGS Service<br/>External API Client"]
        RepoImpl["ğŸ“Š Repository Implementation<br/>SQL Alchemy"]
    end

    %% Connections
    Client -.->|HTTP/WS| API
    Client -.->|WebSocket| WS
    API --> Auth
    API --> DTO
    DTO --> UC
    WS --> Events
    UC --> Entities
    UC --> Events
    Events --> DomainEvents
    Entities --> Repository
    Repository --> RepoImpl
    RepoImpl --> PostgreSQL
    UC --> USGSService
    USGSService -.->|REST API| USGS
    Events --> WS

    %% Styling
    classDef external fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef presentation fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef application fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef domain fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef infrastructure fill:#fce4ec,stroke:#880e4f,stroke-width:2px

    class USGS,Client external
    class API,WS,Auth presentation
    class UC,Events,DTO application
    class Entities,DomainEvents,Repository domain
    class PostgreSQL,USGSService,RepoImpl infrastructure
```

### **Data Flow Architecture**

#### **Real-time Data Ingestion Flow**

```mermaid
sequenceDiagram
    participant USGS as ğŸŒ USGS API<br/>earthquake.usgs.gov
    participant Scheduler as â° Scheduler/Trigger<br/>ingestion.router<br/>Manual/Cron Job
    participant USGSService as ğŸ”Œ USGS Service<br/>USGSService<br/>httpx.AsyncClient
    participant UseCase as âš™ï¸ Ingestion Use Case<br/>IngestEarthquakeDataUseCase<br/>ScheduledIngestionUseCase
    participant Repository as ğŸ“Š Repository<br/>EarthquakeRepository<br/>PostgreSQLEarthquakeRepository
    participant PostgreSQL as ğŸ—„ï¸ PostgreSQL<br/>earthquakes table<br/>PostGIS functions
    participant Events as ğŸ“¨ Event Publisher<br/>InMemoryEventPublisher<br/>EarthquakeEventHandlers
    participant WebSocket as ğŸ“¡ WebSocket Manager<br/>WebSocketManager<br/>broadcast methods
    participant Client as ğŸ’» Frontend Client<br/>WebSocket connection<br/>Real-time UI

    Note over USGS,Client: Real-time Earthquake Data Ingestion Flow

    %% Data Ingestion
    Scheduler->>+USGSService: fetch_recent_earthquakes()
    USGSService->>+USGS: GET /summary/all_day.geojson
    USGS-->>-USGSService: GeoJSON earthquake data
    USGSService-->>-Scheduler: List[Earthquake entities]

    Scheduler->>+UseCase: execute(earthquakes)
    loop For each earthquake
        UseCase->>+Repository: save(earthquake)
        Repository->>+PostgreSQL: INSERT INTO earthquakes
        PostgreSQL-->>-Repository: earthquake_id
        Repository-->>-UseCase: earthquake_id

        %% Domain Events
        UseCase->>+Events: publish(EarthquakeDetected)
        alt If magnitude >= 5.0
            UseCase->>Events: publish(HighMagnitudeAlert)
        end

        Events->>+WebSocket: broadcast_earthquake_update()
        WebSocket->>Client: Real-time notification
        WebSocket-->>-Events: sent
        Events-->>-UseCase: events published
    end
    UseCase-->>-Scheduler: IngestionResult

    Note over Client: Client receives real-time earthquake updates via WebSocket
```

### **REST API Request Flow**

```mermaid
sequenceDiagram
    participant Client as ğŸ’» Frontend Client<br/>React/Vue/Mobile App
    participant API as ğŸ”Œ FastAPI<br/>earthquakes.router<br/>auth.router
    participant Auth as ğŸ” Auth Service<br/>SecurityService<br/>UserRepository
    participant DTO as ğŸ“¦ DTO Layer<br/>EarthquakeFilters<br/>PaginationParams
    participant UseCase as âš™ï¸ Use Case<br/>GetEarthquakesUseCase<br/>GetEarthquakeDetailsUseCase
    participant Repository as ğŸ“Š Repository<br/>EarthquakeRepository<br/>PostgreSQLEarthquakeRepository
    participant PostgreSQL as ğŸ—„ï¸ PostgreSQL<br/>earthquakes table<br/>users table

    Note over Client,PostgreSQL: Typical REST API Request Flow

    %% Authentication
    Client->>+API: POST /api/v1/auth/login
    API->>+Auth: validate_credentials()
    Auth->>+Repository: find_user_by_email()
    Repository->>+PostgreSQL: SELECT * FROM users
    PostgreSQL-->>-Repository: user_data
    Repository-->>-Auth: User entity
    Auth-->>-API: JWT tokens
    API-->>-Client: {access_token, refresh_token}

    %% REST API Request
    Client->>+API: GET /api/v1/earthquakes?magnitude=5.0
    API->>+Auth: verify_token()
    Auth-->>-API: token_valid

    API->>+DTO: create_filter_dto(request)
    DTO-->>-API: EarthquakeFilters

    API->>+UseCase: get_earthquakes(filters, pagination)
    UseCase->>+Repository: find_with_filters()
    Repository->>+PostgreSQL: SELECT * FROM earthquakes WHERE...
    PostgreSQL-->>-Repository: earthquake_records
    Repository-->>-UseCase: List[Earthquake entities]
    UseCase-->>-API: PaginatedResult

    API-->>-Client: {earthquakes: [...], pagination: {...}}

    Note over Client: Client receives filtered earthquake data via REST API
```

### **Clean Architecture Layers**

```
ğŸ“¦ earthquake-monitor/
â”œâ”€â”€ ğŸ¯ src/domain/              # Core Business Logic (innermost layer)
â”‚   â”œâ”€â”€ entities/               # Domain entities (Earthquake, Location, Magnitude)
â”‚   â”œâ”€â”€ repositories/           # Repository interfaces
â”‚   â”œâ”€â”€ services/               # Domain services
â”‚   â”œâ”€â”€ events/                 # Domain events
â”‚   â””â”€â”€ exceptions.py           # Domain-specific exceptions
â”œâ”€â”€ ğŸ”„ src/application/         # Application Layer
â”‚   â”œâ”€â”€ use_cases/              # Application use cases
â”‚   â”œâ”€â”€ dto/                    # Data transfer objects
â”‚   â””â”€â”€ events/                 # Event handlers and publishers
â”œâ”€â”€ ğŸ”Œ src/infrastructure/      # Infrastructure Layer (outermost)
â”‚   â”œâ”€â”€ database/               # Database models and config
â”‚   â”œâ”€â”€ repositories/           # Repository implementations
â”‚   â””â”€â”€ external/               # External API clients (USGS)
â””â”€â”€ ğŸŒ src/presentation/        # Presentation Layer
    â”œâ”€â”€ routers/                # FastAPI route handlers
    â”œâ”€â”€ schemas/                # Pydantic models for API
    â”œâ”€â”€ auth/                   # Authentication logic
    â””â”€â”€ main.py                 # FastAPI application
```

### **Domain-Driven Design**

- **ğŸ¢ Entities**: `Earthquake`, `Location`, `Magnitude` with rich business logic
- **ğŸ“‹ Value Objects**: Immutable objects like `Location` and `Magnitude`
- **ğŸ—‚ï¸ Repositories**: Abstract interfaces for data persistence
- **âš™ï¸ Services**: Domain services for complex business operations
- **ğŸ“¨ Events**: Domain events for decoupled communication

### **Business Logic Separation** (Interview Showcase)

This project demonstrates **proper separation of business logic** across Clean Architecture layers, showing different types of business rules:

#### **ğŸ¯ Domain Layer Business Logic** (Core Business Rules)
**What earthquakes ARE** - Universal truths independent of any application

```python
# src/domain/entities/magnitude.py
class Magnitude:
    def is_significant(self) -> bool:
        """Seismological fact: magnitude â‰¥5.0 is significant."""
        return self.value >= 5.0

    def get_alert_level(self) -> str:
        """Scientific classification - universal truth."""
        if self.value >= 7.0: return "CRITICAL"     # Great earthquakes
        elif self.value >= 5.5: return "HIGH"       # Major earthquakes
        elif self.value >= 4.0: return "MEDIUM"     # Light earthquakes
        return "LOW"                                # Micro earthquakes

# src/domain/entities/earthquake.py
class Earthquake:
    def requires_immediate_alert(self) -> bool:
        """Core business rule: significant earthquakes near populated areas need alerts."""
        return (
            self.magnitude.is_significant() and
            self.location.is_near_populated_area()
        )

    def calculate_affected_radius_km(self) -> float:
        """Physics-based calculation - always true regardless of application."""
        base_radius = self.magnitude.value * 20  # Seismological formula
        depth_factor = max(0.1, 1 - (self.location.depth / 100))  # Depth impact
        return base_radius * depth_factor
```

#### **ğŸ”„ Application Layer Business Logic** (Workflow Orchestration)
**What to DO with earthquakes** - Application-specific processes and coordination

```python
# src/application/use_cases/ingest_earthquake_data.py
class IngestEarthquakeDataUseCase:
    async def execute(self, earthquakes: list[Earthquake]) -> IngestionResult:
        """Application workflow: how to process earthquake data in THIS system."""
        new_earthquakes = 0

        for earthquake in earthquakes:
            # Application logic: duplicate prevention (system-specific)
            existing = await self.repository.find_by_id(earthquake.id)
            if existing:
                continue  # Skip duplicates - application policy

            # Application workflow: persist earthquake
            earthquake_id = await self.repository.save(earthquake)
            new_earthquakes += 1

            # Application coordination: publish events based on domain rules
            await self._publish_events(earthquake)

        return IngestionResult(new_earthquakes=new_earthquakes)

    async def _publish_events(self, earthquake: Earthquake) -> None:
        """Application workflow: when and how to publish events."""
        # Always publish earthquake detected (application policy)
        earthquake_detected = EarthquakeDetected(...)
        await self.event_publisher.publish(earthquake_detected)

        # Use domain logic to decide, but publishing is application concern
        if earthquake.magnitude.is_significant():  # Domain rule
            high_magnitude_alert = HighMagnitudeAlert(...)
            await self.event_publisher.publish(high_magnitude_alert)  # App workflow

# src/application/use_cases/get_earthquakes.py
class GetEarthquakesUseCase:
    async def execute(self, filters: EarthquakeFilters, pagination: PaginationParams):
        """Application logic: how to retrieve and present earthquake data."""
        # Application workflow: coordinate filtering with repository
        earthquakes = await self.repository.find_with_filters(
            filters, pagination.offset, pagination.limit
        )

        # Application logic: calculate pagination metadata
        total = await self.repository.count_with_filters(filters)

        return PaginatedResult(
            items=earthquakes,
            total=total,
            page=pagination.page,
            size=pagination.size,
            pages=(total + pagination.size - 1) // pagination.size  # App calculation
        )
```

#### **ğŸ“Š Comparison Table**

| **Domain Logic** | **Application Logic** |
|------------------|----------------------|
| `magnitude.is_significant()` | "When to publish events" |
| `earthquake.requires_immediate_alert()` | "How to handle duplicates" |
| `calculate_affected_radius_km()` | "Pagination and filtering workflows" |
| **Pure business rules** | **Coordination and orchestration** |
| **Technology-independent** | **Uses repositories, events, external services** |
| **Always true (physics/science)** | **Application-specific policies** |
| **What earthquakes ARE** | **What to DO with earthquakes** |

#### **ğŸ­ Real-world Interview Analogy**
- **Domain Logic** = **Doctor's Medical Knowledge**: "Blood pressure > 140/90 is hypertension" (always true)
- **Application Logic** = **Hospital Workflow**: "Send SMS alerts to emergency contacts for critical patients" (hospital-specific process)

This separation ensures:
- **Domain rules** can be tested in isolation
- **Business logic** is technology-independent
- **Application workflows** can change without affecting core business rules
- **Clean boundaries** between what the business does vs. how the system does it

### **Key Design Patterns**

- **ğŸ”„ Repository Pattern**: Abstract data access
- **ğŸ¯ Use Case Pattern**: Application-specific business workflows
- **ğŸ­ Factory Pattern**: Object creation abstraction
- **ğŸ“¢ Observer Pattern**: Event-driven architecture
- **ğŸ›¡ï¸ Dependency Injection**: Loose coupling between layers

## âš™ï¸ Configuration

### **Environment Variables**

```bash
# Database Configuration
DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/earthquake_monitor
DATABASE_POOL_SIZE=10
DATABASE_MAX_OVERFLOW=20

# Authentication
JWT_SECRET_KEY=your-super-secret-jwt-key
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30

# USGS API Configuration
USGS_API_BASE_URL=https://earthquake.usgs.gov/fdsnws/event/1
USGS_GEOJSON_BASE_URL=https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary
USGS_API_TIMEOUT=30
USGS_POLLING_INTERVAL=300

# Security
ALLOWED_ORIGINS=http://localhost:3000,https://app.ndgms.org
ALLOWED_HOSTS=localhost,127.0.0.1,api.ndgms.org

# Monitoring
LOG_LEVEL=INFO
SENTRY_DSN=your-sentry-dsn
```

### **Database Configuration**

The system uses PostgreSQL with the following optimizations:
- **Connection Pooling**: SQLAlchemy async connection pool
- **Migrations**: Alembic for database versioning
- **Indexing**: Optimized indexes for query performance
- **PostGIS Integration**: Full spatial database capabilities with ST_DWithin, ST_Distance, and geometry indexing

## ğŸ› ï¸ Development

### **Development Setup**

```bash
# Install development dependencies
poetry install --with dev

# Setup pre-commit hooks
pre-commit install

# Run in development mode
uvicorn src.presentation.main:app --reload --log-level debug
```

### **Code Quality Tools**

```bash
# Format code
black src/ tests/
isort src/ tests/

# Lint code
ruff check src/ tests/

# Type checking
mypy src/

# Run all quality checks
make lint

# Auto-fix common issues
make format
```

### **Database Operations**

```bash
# Create new migration
alembic revision --autogenerate -m "Add new feature"

# Apply migrations (includes PostGIS setup)
alembic upgrade head

# Rollback migration
alembic downgrade -1

# Check migration status
alembic current
```

### **ğŸŒ USGS Data Integration**

The system integrates with USGS (United States Geological Survey) earthquake data through two complementary methods:

#### **1. GeoJSON Feed Service** (Primary - Real-time)
- **Endpoint**: `https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/{magnitude}_{period}.geojson`
- **Usage**: Real-time monitoring with scheduled ingestion
- **Data**: Pre-built feeds with different magnitude thresholds and time periods
- **Examples**:
  - `all_day.geojson` - All earthquakes in the past 24 hours
  - `4.5_week.geojson` - Magnitude 4.5+ earthquakes in the past week
  - `significant_month.geojson` - Significant earthquakes in the past month

#### **2. FDSNWS Query API** (Historical Data)
- **Endpoint**: `https://earthquake.usgs.gov/fdsnws/event/1/query`
- **Usage**: Flexible historical data retrieval and bulk imports
- **Features**: Custom date ranges, precise magnitude filtering, multiple output formats
- **Parameters**: `starttime`, `endtime`, `minmagnitude`, `format=geojson`

#### **Data Processing Pipeline**
```mermaid
flowchart LR
    A[ğŸŒ USGS API] --> B[ğŸ”Œ USGS Service]
    B --> C[ğŸ“Š Data Validation]
    C --> D[ğŸ¢ Domain Entities]
    D --> E[ğŸ“¥ Repository Layer]
    E --> F[ğŸ—„ï¸ PostgreSQL + PostGIS]
    D --> G[ğŸ“¨ Domain Events]
    G --> H[ğŸ“¡ WebSocket Broadcast]
    H --> I[ğŸ’» Real-time Clients]
```

### **ğŸš¨ High-Magnitude Alert System**

The system implements an intelligent alert system based on earthquake significance:

#### **Magnitude Classification**
- **Significant Earthquakes**: Magnitude â‰¥ 5.0
- **Alert Levels**:
  - ğŸŸ¢ **LOW**: < 4.0 (Minor impact)
  - ğŸŸ¡ **MEDIUM**: 4.0 - 5.4 (Moderate impact)
  - ğŸŸ  **HIGH**: 5.5 - 6.9 (Major impact)
  - ğŸ”´ **CRITICAL**: â‰¥ 7.0 (Extreme impact)

#### **Alert Triggers**
- **High-Magnitude Alert**: Automatically triggered for earthquakes â‰¥ 5.0
- **Immediate Response Required**: Earthquakes â‰¥ 5.0 near populated areas
- **Impact Radius**: Calculated as `magnitude Ã— 20km Ã— depth_factor`

#### **Real-time Notifications**
```json
{
  "type": "high_magnitude_alert",
  "data": {
    "earthquake_id": "us6000abcd",
    "magnitude": 7.2,
    "alert_level": "CRITICAL",
    "affected_radius_km": 144.0,
    "requires_immediate_response": true,
    "latitude": 35.5,
    "longitude": -120.5
  }
}
```

### **ğŸ“¡ WebSocket Real-time Features**

#### **Connection & Authentication**
- **Endpoint**: `ws://localhost:8000/api/v1/ws`
- **Authentication**: JWT token required for connection
- **Auto-reconnection**: Client-side reconnection handling recommended

#### **Subscription Types**
```json
// Subscribe to all new earthquake notifications
{"action": "subscribe_earthquakes"}

// Subscribe to high-magnitude alerts only (â‰¥5.0)
{"action": "subscribe_alerts"}
```

#### **Real-time Message Types**

**1. Earthquake Detection** (All new earthquakes)
```json
{
  "type": "earthquake_detected",
  "data": {
    "id": "us6000abcd",
    "magnitude": 4.2,
    "latitude": 35.5,
    "longitude": -120.5,
    "depth": 10.5,
    "occurred_at": "2024-01-15T14:30:00Z",
    "source": "USGS",
    "timestamp": "2024-01-15T14:32:15Z"
  }
}
```

**2. High-Magnitude Alert** (Significant earthquakes â‰¥5.0)
```json
{
  "type": "high_magnitude_alert",
  "data": {
    "earthquake_id": "us6000abcd",
    "magnitude": 6.8,
    "alert_level": "HIGH",
    "affected_radius_km": 95.2,
    "requires_immediate_response": true,
    "timestamp": "2024-01-15T14:32:15Z"
  }
}
```

**Note**: Currently, WebSocket subscriptions do not support severity filtering. Clients receive all events for their subscription type and should filter client-side based on alert levels.

#### **Manual Data Ingestion**
```bash
# Using the standalone script (FDSNWS API)
python scripts/ingestion/usgs_ingestion.py

# Using the API endpoint (GeoJSON feeds)
curl -X POST "http://localhost:8000/api/v1/ingestion/trigger" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "source": "USGS",
    "period": "day",
    "magnitude_filter": "2.5",
    "limit": 100
  }'
```

#### **Ingestion Configuration**
```bash
# Environment variables for USGS integration
USGS_API_BASE_URL=https://earthquake.usgs.gov/fdsnws/event/1
USGS_GEOJSON_BASE_URL=https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary
USGS_API_TIMEOUT=30
USGS_POLLING_INTERVAL=300
```

#### **Available USGS Feed Options**
- **Time Periods**: `hour`, `day`, `week`, `month`
- **Magnitude Filters**: `all`, `significant`, `4.5`, `2.5`, `1.0`
- **Data Sources**: Real-time GeoJSON feeds updated every 5 minutes

## ğŸ§ª Testing

### **Running Tests**

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test categories
pytest tests/unit/          # Unit tests only
pytest tests/integration/   # Integration tests only

# Run tests in parallel
pytest -n auto

# Run tests with verbose output
pytest -v
```

### **Test Categories**

- **ğŸ“ Unit Tests** (42 tests): Test individual components in isolation
- **ğŸ”— Integration Tests** (45 tests): Test component interactions
- **ğŸŒ API Tests**: End-to-end API functionality
- **ğŸ—„ï¸ Database Tests**: Database operations and migrations

### **Test Coverage**

Current test coverage: **95%+**

```bash
# Generate coverage report
pytest --cov=src --cov-report=html
open htmlcov/index.html
```

## ğŸš€ Deployment

### **Docker Deployment**

```bash
# Build production image
docker build -f docker/Dockerfile -t earthquake-monitor:latest .

# Run with Docker Compose
docker-compose -f docker/docker-compose.yml up -d

# Check service status
docker-compose ps
docker-compose logs earthquake-monitor
```

### **Environment-Specific Configurations**

- **ğŸ”§ Development**: `docker-compose.dev.yml` with hot reload
- **ğŸ§ª Testing**: In-memory database for fast tests
- **ğŸš€ Production**: Optimized with proper security and monitoring

### **Health Checks**

The application includes comprehensive health checks:

```bash
# Application health
curl http://localhost:8000/health

# Database connectivity
curl http://localhost:8000/health/db

# External service status
curl http://localhost:8000/health/external
```

### **Monitoring & Logging**

- **ğŸ“Š Structured Logging**: JSON logs with request tracing
- **ğŸ“ˆ Metrics**: Request/response times, error rates
- **ğŸš¨ Alerting**: Health check failures and error thresholds
- **ğŸ” Tracing**: Request correlation IDs

## ğŸ“ˆ Performance

### **Optimization Features**

- **âš¡ Async/Await**: Non-blocking I/O throughout
- **ğŸŠ Connection Pooling**: Efficient database connections
- **ğŸ“„ Pagination**: Memory-efficient large dataset handling
- **ğŸ—‚ï¸ Database Indexing**: Optimized query performance
- **ğŸ’¾ Future Caching**: Redis integration planned

### **Scalability Considerations**

- **ğŸ”„ Horizontal Scaling**: Stateless application design
- **ğŸ“Š Load Balancing**: Ready for multiple instances
- **ğŸ—„ï¸ Database Scaling**: Read replicas and sharding support
- **ğŸ“¡ Message Queuing**: Event-driven architecture for async processing

## ğŸ¤ Contributing

### **Development Workflow**

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature/amazing-feature`
3. **Install** development dependencies: `poetry install --with dev`
4. **Make** your changes with proper tests
5. **Run** quality checks: `make lint test`
6. **Commit** with conventional commits: `git commit -m "feat: add amazing feature"`
7. **Push** to your branch: `git push origin feature/amazing-feature`
8. **Create** a Pull Request

### **Code Standards**

- **ğŸ¯ Clean Architecture**: Follow established patterns
- **ğŸ“ Documentation**: Document all public APIs
- **ğŸ§ª Testing**: Maintain 95%+ test coverage
- **ğŸ” Type Hints**: Full type annotation required
- **ğŸ“ Code Style**: Black + isort + ruff compliance

### **Pull Request Process**

1. Ensure all tests pass and coverage remains high
2. Update documentation for new features
3. Add integration tests for new endpoints
4. Follow semantic versioning for changes
5. Request review from maintainers

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **[USGS](https://earthquake.usgs.gov/)** - Earthquake data provider
- **[FastAPI](https://fastapi.tiangolo.com/)** - Modern Python web framework
- **[SQLAlchemy](https://www.sqlalchemy.org/)** - Database ORM
- **[Pydantic](https://pydantic-docs.helpmanual.io/)** - Data validation
- **Clean Architecture** - Robert C. Martin's architectural pattern

---

## ğŸ”— Related Projects

- **[NDGMS](../README.md)** - Natural Disaster Global Monitoring System

---
